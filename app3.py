import streamlit as st
import os
import openai
import weaviate
import base64
import re
from sentence_transformers import SentenceTransformer
from weaviate.classes.init import Auth

# Configura tu API Key de OpenAI
openai.api_key = "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"  # Aseg√∫rate de manejar la API Key de forma segura

# Best practice: store your credentials in environment variables
weaviate_url = os.environ["WEAVIATE_URL"]  # Ahora est√°s buscando "WEAVIATE_URL"
weaviate_api_key = os.environ["WEAVIATE_API_KEY"]  # Ahora est√°s buscando "WEAVIATE_API_KEY"

client = weaviate.connect_to_weaviate_cloud(
    cluster_url=weaviate_url,
    auth_credentials=Auth.api_key(weaviate_api_key),
)

"""# Conectar a Weaviate local
client = weaviate.connect_to_local(host="localhost", port=8080)
collection = client.collections.get("Noroto")"""

# Cargar modelo de embeddings
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Funci√≥n para obtener embedding
def get_embedding(text):
    return embedding_model.encode(text).tolist()

# Recuperar chunks similares
def retrieve_similar_chunks(query, k=5):
    query_vector = get_embedding(query)
    results = collection.query.near_vector(near_vector=query_vector, limit=k)

    context = []
    for obj in results.objects:
        properties = obj.properties
        context.append({
            "text": properties.get("text", ""),
            "image": properties.get("image", None),
            "page_number": properties.get("page_number", -1),
            "source": properties.get("source", "")
        })
    return context

# Eliminar chunks duplicados por texto y p√°gina
def remove_duplicate_chunks(chunks):
    seen = set()
    unique_chunks = []
    for chunk in chunks:
        key = (chunk["page_number"], chunk["text"].strip())
        if key not in seen:
            seen.add(key)
            unique_chunks.append(chunk)
    return unique_chunks

# Agrupar por p√°gina (opcional, si quieres agrupar m√∫ltiples chunks por p√°gina)
def group_chunks_by_page(chunks):
    grouped = {}
    for chunk in chunks:
        page = chunk["page_number"]
        grouped.setdefault(page, {"texts": [], "image": chunk.get("image")})
        grouped[page]["texts"].append(chunk["text"])
    return grouped

# Generar respuesta y filtrar chunks usados
def generate_response(query, context):
    context_text = "\n\n".join([f"[P√°gina {c['page_number']}]: {c['text']}" for c in context])

    prompt = f"""Responde la siguiente pregunta usando exclusivamente el contexto provisto.
Si no puedes responder con certeza, di "No lo s√©".

CONTEXT:
{context_text}

PREGUNTA:
{query}

Despu√©s de la respuesta, indica solo los n√∫meros de p√°gina utilizados para responder de forma clara y concisa (ej. P√ÅGINAS UTILIZADAS: 2, 3).

RESPUESTA:"""

    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )

    response_text = response.choices[0].message["content"]

    # Extraer p√°ginas utilizadas
    match = re.search(r"P√ÅGINAS UTILIZADAS:\s*([\d, ]+)", response_text)
    used_pages = []
    if match:
        page_numbers_str = match.group(1)
        used_pages = [int(p.strip()) for p in page_numbers_str.split(",") if p.strip().isdigit()]
        # Quitar esa l√≠nea del texto mostrado
        response_text = re.sub(r"P√ÅGINAS UTILIZADAS:\s*[\d, ]+", "", response_text).strip()

    # Filtrar chunks por p√°ginas usadas
    used_chunks = [c for c in context if c["page_number"] in used_pages]
    unique_chunks = remove_duplicate_chunks(used_chunks)

    return response_text, unique_chunks

# Streamlit UI
st.set_page_config(page_title="Chat con Nori", page_icon="ü§ñ")
st.markdown("<h1 style='text-align: center;'>Chat con Nori ü§ñ</h1>", unsafe_allow_html=True)
st.write("Preg√∫ntale sobre el Manual de Procedimientos de Norauto")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

user_input = st.chat_input("Escribe tu pregunta...")
if user_input:
    with st.spinner("Buscando contexto..."):
        context = retrieve_similar_chunks(user_input)

    with st.spinner("Consultando al modelo..."):
        respuesta, used_chunks = generate_response(user_input, context)

    st.session_state.chat_history.append({
        "role": "user",
        "content": user_input
    })
    st.session_state.chat_history.append({
        "role": "assistant",
        "content": respuesta,
        "context": used_chunks
    })

# Mostrar historial del chat
for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if msg["role"] == "assistant" and "context" in msg:
            st.divider()
            st.markdown("## Fuentes utilizadas:")
            grouped = group_chunks_by_page(msg["context"])
            for page_num, data in grouped.items():
                with st.expander(f"üìÑ Fuente: P√°gina {page_num}"):
                    for txt in data["texts"]:
                        st.markdown(f"- {txt}")
                if data["image"]:
                    with st.expander(f"üñºÔ∏è Imagen asociada a P√°g {page_num}"):
                        image_bytes = base64.b64decode(data["image"])
                        st.image(image_bytes, caption=f"Imagen - P√°gina {page_num}")
